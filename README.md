# LLM+NCL
Large Language Models (LLMs) are amazing at generating text, but often brittle at reasoning. Ask them to solve a math word problem, and they may output a polished explanation — that’s sometimes completely wrong.
This project combines Qwen/Qwen2.5-1.5B-Instruct with ACT-R-inspired reasoning traces, creating a Neural Cognitive Architecture (NCA) Fusion model that significantly improves reasoning accuracy. The source code and dataset are available in the Colab Notebook.
