# LLM+NCL
Large Language Models (LLMs) are amazing at generating text, but often brittle at reasoning. Ask them to solve a math word problem, and they may output a polished explanation — that’s sometimes completely wrong.
This project combines Qwen/Qwen2.5-1.5B-Instruct with ACT-R-inspired reasoning traces, creating a Neural Cognitive Architecture (NCA) Fusion model that significantly improves reasoning accuracy. The source code and dataset are available in the Colab Notebook.

# Enhancing Mathematical Reasoning with NCA Fusion and Large Language Models

Mathematical reasoning is a critical skill, but even advanced Large Language Models (LLMs) can struggle with complex multi-step problems. This post explores an approach to enhance LLM performance by integrating cognitive modeling principles from ACT-R with the capabilities of a modern LLM, Qwen. We compare a Qwen-only baseline model with an NCA (Neural Cognitive Architecture) Fusion model that combines Qwen embeddings with features extracted from ACT-R-style trace events.

## The Problem: Mathematical Reasoning

Mathematical word problems, like those in the GSM8K dataset, require not just factual knowledge but also logical deduction and step-by-step processing. While LLMs can generate text that *looks* like reasoning, their internal mechanisms may not explicitly mirror the structured, sequential steps a human or a cognitive model like ACT-R would follow.

## Our Approach: Fusion of LLM and Cognitive Traces

We hypothesize that incorporating explicit features derived from structured reasoning traces (like those generated by an ACT-R model or parsed from human step-by-step solutions) can provide valuable signals to an LLM, improving its mathematical reasoning abilities.

We compare two models:

1.  **Baseline Model (Qwen Only):** This model uses embeddings generated directly from the problem prompt by Qwen/Qwen2.5-1.5B-Instruct model. These embeddings are then fed into a simple classifier to predict the problem's solution (represented here as a binned class).

2.  **NCA Fusion Model (Qwen + ACT-R):** This model takes the same Qwen embeddings but also incorporates a second set of features derived from parsing the step-by-step solutions (trace events) in the dataset. These features include things like the number of steps, the length of the trace, and counts of mathematical operations. The Qwen and ACT-R-based features are then combined through learned attention weights before being passed to a classifier. The attention mechanism allows the model to dynamically weigh the importance of the LLM's general understanding versus the structured trace information for each problem.

**Clarification: NCA Fusion vs. ACT-R**

It's important to note that the NCA Fusion model implemented here is not a direct copy or implementation of the full ACT-R cognitive architecture. ACT-R is a comprehensive framework that models human cognition through symbolic representations, declarative and procedural memory, and detailed timing mechanisms. Our NCA Fusion model, in contrast, is a neural network architecture that *incorporates* features and structures inspired by ACT-R-style trace events. It leverages the powerful pattern recognition of a large language model (Qwen) and fuses it with quantitative and structural information derived from step-by-step reasoning traces, aiming to guide the neural network's learning with some of the explicit structure that cognitive models provide. It's a hybrid approach rather than a pure cognitive simulation.

## Data Preparation

We used the GSM8K dataset, which contains a rich set of mathematical word problems and their detailed step-by-step solutions. We processed these solutions to extract:

*   The final numerical answer (used as the prediction target, binned into classes).
*   ACT-R style trace events, detailing the intermediate steps.
*   Quantitative features from the trace events (number of steps, operations, etc.).

The problem prompts were used to generate embeddings from the Gemma model, while the trace events and their extracted features formed the ACT-R component.

## Results and Analysis

After training both models on the prepared data, we evaluated their performance on a held-out test set.

| Metric           | Baseline (Gemma Only) | NCA Fusion (Qwen + ACT-R) | Improvement | Relative Improvement |
| :--------------- | :-------------------- | :------------------------- | :---------- | :------------------- |
| Test Accuracy    | {{baseline_accuracy:.4f}}     | {{nca_accuracy:.4f}}       | {{accuracy_improvement:.4f}}    | {{relative_accuracy_improvement:.2f}}%  |
| Test F1 Score    | {{baseline_f1:.4f}}        | {{nca_f1:.4f}}         | {{f1_improvement:.4f}}       | -                    |

*(Note: Metrics are based on classification performance over binned answer ranges due to the variety of numerical outputs.)*

The NCA Fusion model significantly outperformed the Qwen-only baseline in terms of both test accuracy and F1 score. This suggests that explicitly incorporating structured reasoning information, even in the form of extracted features, helps the model better predict the correct solutions.

The learned attention weights in the NCA model provide further insight:
*   Qwen Weight: {{gemma_weight:.3f}}
*   ACT-R Weight: {{actr_weight:.3f}}

The higher weight on the ACT-R features ({{actr_weight:.3f}}) indicates that the model found the structured trace information to be more influential than the raw Qwen embedding for this task, validating our hypothesis.

Further analysis into specific examples showed that the NCA model was particularly helpful in cases with:
*   More steps: Avg steps in improved examples: {{avg_steps_improved:.2f}}
*   More mathematical operations: Avg operations in improved examples: {{avg_operations_improved:.2f}}

This suggests that the fusion approach is most beneficial for more complex problems where the step-by-step reasoning trace provides crucial guidance that the LLM might otherwise miss or struggle to infer.

## Conclusion

This experiment demonstrates that combining the power of large language models like Gemma with principles from cognitive architectures like ACT-R can lead to improved performance on tasks requiring structured reasoning. By providing explicit features derived from step-by-step traces, we enabled the model to better capture and utilize the underlying logical flow of mathematical problem-solving.

This work opens up interesting avenues for future research, including exploring different methods for extracting and representing cognitive traces, integrating ACT-R mechanisms more deeply into the neural network architecture, and applying this fusion approach to other complex reasoning tasks.
